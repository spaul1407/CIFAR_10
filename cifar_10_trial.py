# -*- coding: utf-8 -*-
"""cifar-10_trial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_gVG7ZwvbHXA83o9CrE_ERsDfgbj9LEu
"""

import torch
from torch import nn
import torchvision
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10
import matplotlib.pyplot as plt

device='cuda' if torch.cuda.is_available() else 'cpu'
device

trainset=CIFAR10(root='./data',
                 train=True,
                 download=True,
                 transform=torchvision.transforms.ToTensor(),
                 target_transform=None)
testset=CIFAR10(root='./data',
                train=False,
                download=True,
                transform=torchvision.transforms.ToTensor(),
                target_transform=None)

classes=trainset.classes
classes

#torch.manual_seed(42)
fig=plt.figure(figsize=(9,9))

rows, cols= 3,3
for i in range(1,rows*cols+1):
  index=torch.randint(0,len(trainset),size=[1]).item()
  image, label=trainset[index]
  fig.add_subplot(rows,cols,i)
  plt.imshow(image.permute(1,2,0))
  plt.title(classes[label])
  plt.axis(False)

from torch.utils.data import DataLoader

#Hyper-parameters
BATCH_SIZE=32
k=32
compression_factor=0.5

train_dataloader=DataLoader(dataset=trainset,
                            shuffle=True,
                            batch_size=BATCH_SIZE)
test_dataloader=DataLoader(dataset=testset,
                           shuffle=False,
                           batch_size=BATCH_SIZE)

len(train_dataloader),len(test_dataloader)

train_features_batch, train_labels_batch=next(iter(train_dataloader))
train_features_batch, train_labels_batch=train_features_batch.to(device),train_labels_batch.to(device)

train_features_batch.shape

class DenseLayer(nn.Module):
  def __init__(self, input_shape):
    super().__init__()
    self.layer1=nn.Sequential(
        nn.BatchNorm2d(num_features=input_shape),
        nn.Conv2d(in_channels=input_shape,
                  out_channels=4*k,
                  kernel_size=1,
                  stride=1,
                  padding=1),
        nn.ReLU()
    )
    self.layer2=nn.Sequential(
        nn.BatchNorm2d(num_features=4*k),
        nn.Conv2d(in_channels=4*k,
                  out_channels=k,
                  kernel_size=3,
                  stride=1,
                  padding=1),
        nn.ReLU()
    )
  def forward(self,x):
    x_input=x
    x=self.layer2(self.layer1(x))
    x=torch.cat([x_input,x],dim=1)
    return x

class DenseBlock(nn.Module):
    def __init__(self,layers,input_shape):
      super().__init__()
      self.layers = layers
      self.block1 = nn.ModuleList()
      for num in range(self.layers):
        self.block1.add_module(f"DenseLayer_{num}",DenseLayer(input_shape+k*num))

    def forward(self,x):
      x_input = x
      for layer in self.block1:
        x = layer(x)

      return x

class TransitionLayer(nn.Module):
    def __init__(self,input_shape,compression_factor):
      super().__init__()
      self.transition_layer=nn.Sequential(
          nn.BatchNorm2d(input_shape),
          nn.Conv2d(in_channels = input_shape ,
                    out_channels = int(input_shape*compression_factor),
                    kernel_size = 1 ,
                    stride = 1,
                    padding = 1),
          nn.AvgPool2d(kernel_size=2)
      )

    def forward(self,x):
      x = self.transition_layer(x)
      return x

model_transitionLayer=TransitionLayer(input_shape=3,
                                      compression_factor=compression_factor)

class DenseNet(nn.Module):
    def __init__(self,input_shape,densenet_variant,num_classes=len(classes)):
      super().__init__()
      self.conv1 = nn.Conv2d(in_channels=input_shape ,
                             out_channels=64,
                             kernel_size=7,
                             stride=2,
                             padding=3)
      self.BatchNorm1 = nn.BatchNorm2d(num_features=64)
      self.relu = nn.ReLU()
      self.maxpool = nn.MaxPool2d(kernel_size=2,
                                  stride=2)


      # adding 3 DenseBlocks and 3 Transition Layers
      self.block1 = nn.ModuleList()
      dense_block_inchannels = 64

      for num in range(len(densenet_variant))[:-1]:

            self.block1.add_module( f"DenseBlock_{num+1}" , DenseBlock(densenet_variant[num] , dense_block_inchannels ) )
            dense_block_inchannels  = int(dense_block_inchannels + k*densenet_variant[num])

            self.block1.add_module( f"TransitionLayer_{num+1}" , TransitionLayer( dense_block_inchannels,compression_factor ) )
            dense_block_inchannels = int(dense_block_inchannels*compression_factor)

        # adding the 4th and final DenseBlock
      self.block1.add_module( f"DenseBlock_{num+2}" , DenseBlock( densenet_variant[-1] , dense_block_inchannels ) )
      dense_block_inchannels  = int(dense_block_inchannels + k*densenet_variant[-1])

      self.BatchNorm2 = nn.BatchNorm2d(num_features=dense_block_inchannels)
      self.avg_pool = nn.AdaptiveAvgPool2d(1)

      self.linear_final = nn.Linear(in_features=dense_block_inchannels,
                                    out_features=num_classes)


    def forward(self,x):

      x = self.maxpool(self.relu(self.BatchNorm1(self.conv1(x))))

      for layer in self.block1:
            x = layer(x)

      x = self.relu(self.BatchNorm2(x))
      x = self.avg_pool(x)

      x = torch.flatten(x, start_dim=1)
      x = self.linear_final(x)

      return x


modelCIFAR10=DenseNet(input_shape=3,densenet_variant=[6,12,24,16])  #using the densenet121



